import os
import tarfile
import requests
import json
import numpy as np
import glob
from sklearn.feature_extraction.text import CountVectorizer
import ember
from keras.models import Sequential
from keras.layers import Dense, BatchNormalization

# Set up file paths and URLs
DATA_DIR = "ember_dataset"
JSONL_FILE = "ember_dataset_2018_2.tar.bz2"
JSONL_URL = "https://ember.elastic.co/ember_dataset_2018_2.tar.bz2"
filename = JSONL_URL.split("/")[-1]

def download_dataset():
    # create the data directory if it doesn't exist
    os.makedirs(DATA_DIR, exist_ok=True)

    # download the file
    response = requests.get(JSONL_URL, stream=True)
    if response.status_code == 200:
        with open(os.path.join(DATA_DIR, JSONL_FILE), "wb") as f:
            for chunk in response.iter_content(chunk_size=1024):
                f.write(chunk)

    # extract the file
    with tarfile.open(os.path.join(DATA_DIR, JSONL_FILE), "r:bz2") as tar:
        tar.extractall(DATA_DIR)

def build_model():
    
    # define the file extension and the number of lines to read
    file_extension = ".jsonl"
    num_lines = 100

    # get a list of all .jsonl files in the current directory
    file_0 = "ember2018/train_features_0.jsonl"
    file_1 = "train_features_1.jsonl"
    file_2 = "train_features_2.jsonl"
    file_3 = "train_features_3.jsonl"
    file_4 = "train_features_4.jsonl"
    file_5 = "train_features_5.jsonl"
    #file_list = glob.glob(f"train_features_*{file_extension}")
    data_0 = []

    line_count = 0
    with open(file_0, "rb") as f:
        while line_count < 100:
            data_in = f.readline()
            data_0.append(data_in)
            line_count += 1
    print(len(data_0))

    #ember.create_vectorized_features("./ember2018/")
    X_train, y_train, X_test, y_test = ember.read_vectorized_features("./ember2018/")
    #np.savetxt('X_train.csv', X_train, delimiter=',')
    #np.savetxt('y_train.csv', y_train, delimiter=',')
    #np.savetxt('X_test.csv', X_test, delimiter=',')
    #np.savetxt('y_test.csv', y_test, delimiter=',')\

    numInputFeatures = X_train.shape[1] #2381
    numExamples = X_train.shape[0]
    numOutputs = y_train.shape[0]
    
    # 1 = malicious, 0 = benign, -1 = unlabled
    indices = []
    for i,val in enumerate(y_train):
        if val == -1: 
            indices.append(i)

    test_indicies = []
    for i,val in enumerate(y_test):
        if val == -1:
            test_indicies.append(i)

    delete_indices = np.array(indices)
    delete_test_indices = np.array(test_indicies)

    new_y_train = np.delete(y_train, delete_indices)
    new_X_train = np.delete(X_train, delete_indices, axis=0)
    # new_y_test = np.delete(y_test, delete_test_indices)
    # new_X_test = np.delete(X_test, delete_test_indices, axis=0)
    
    model = Sequential()
    
    #model.add(Dense(numInputFeatures, input_dim=numInputFeatures, activation = 'relu', kernel_initializer= 'he_normal'))
    model.add(Dense(numInputFeatures, input_dim=numInputFeatures, activation = 'relu', kernel_initializer= 'he_normal'))
    model.add(BatchNormalization()) 
    model.add(Dense(10000,activation = 'relu'))
    model.add(Dense(2000,activation = 'relu'))
    model.add(Dense(1500,activation = 'relu'))
    model.add(Dense(1000,activation = 'relu'))
    model.add(Dense(500,activation = 'relu'))
    model.add(Dense(250,activation = 'relu'))
    model.add(Dense(100,activation = 'relu'))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(25,activation = 'relu'))
    model.add(Dense(1, activation='relu'))

    #model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
    model.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])
    #model.compile(loss='sparse_categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])
    #Adagrad, RMSprop, Adadelta, NAdam, SGD
    
    model.summary() # display details of model

    #history_ret = model.fit(X_train, y_train, epochs=100, batch_size = 500, verbose=1) 
    history_ret = model.fit(new_X_train, new_y_train, epochs=500, batch_size = 500, verbose=1) # train - 100 epochs, batch_size = 500
    print(history_ret.history.keys())
    _,accuracy = model.evaluate(X_test, y_test, verbose=1)
    #_,accuracy = model.evaluate(new_X_test, new_y_test, verbose=1)
    print('Accuracy: %.2f' % (accuracy*100))

def main():
    # download the dataset
    #download_dataset()

    # convert the JSONL file to NumPy arrays
    build_model()


main()
